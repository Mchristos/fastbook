{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3b16b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058ca85",
   "metadata": {},
   "source": [
    "# Read input data: digit images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c61b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/mchristos/.fastai/data/mnist_sample/labels.csv'),Path('/home/mchristos/.fastai/data/mnist_sample/valid'),Path('/home/mchristos/.fastai/data/mnist_sample/train')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad907c44",
   "metadata": {},
   "source": [
    "We have a training and validation set of mnist digits 3 and 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7afda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "threePaths = (path/'train'/'3').ls()\n",
    "sevenPaths = (path/'train'/'7').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7855ae",
   "metadata": {},
   "source": [
    "Run below to see a random three or seven image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842a7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8UlEQVR4nMXQv0tCURjG8UfxFxZJUoNBEAQqZX+AuDs3OLhJDtHSDxoaw1mXEPofhAZdGhtFRExaAsfGoCbBm+L94qJ18Z672ru88Hzewznnlf654k+fcS87HsCguGvGLgDfid/A78ChJGl7XweLwOfAWFKJlrRhebzoEWYRs+3UgQezVd6BD7/RTgGw8n+JYy4kSQo3z00nTy4zmfIXjDy3lALKrtSXLpWOpGAPOi68BhpS6BVuVi1nAwXpDMiu4j1Q3TqsTWEYXYaBRe//RLRpPyclXYxdd95Z8DaB0ZVpRYUxYLfT5i/uvdC8NdMaaw6IRWC+PTuWMAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7FEEE262EFA0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomIndex = np.random.randint(len(threePaths))\n",
    "im3=Image.open(threePaths[randomIndex])\n",
    "im3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451591a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAApElEQVR4nGNgGEFA/9+/f//+/fv/79+/f19T+dEk/yKDC0ekoeKMDAwMDCLhcHWNggwMv90OYbeBTeD037+9OB1Q8ve7MpTJhEX63108kqyRuEyVv/H3K04rV/79uxSXnMO3v1M5cMjxbPn7SQmXxpq/X91xSIltfLdfBpe+8L9f/XHJMTz/hMtMBobCX89QBZBCiFGMcTZOjfJ/N6CJYAtb4iQBk4BBEbQbJ/4AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7FEEE2659F70>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomIndex = np.random.randint(len(sevenPaths))\n",
    "im7=Image.open(sevenPaths[randomIndex])\n",
    "im7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fdc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,  18, 171, 233,  18,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  43, 207, 254, 254, 207,  88,  21,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0, 201, 254, 254, 254, 254, 254, 237,  25,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0, 162, 254, 254, 254, 254, 254, 254,  54,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  15,  53,  53,  78, 254, 254, 254,   9,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 149, 254, 254, 252,   8,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  20, 146, 254, 254, 254, 144,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0, 128, 214, 254, 254, 254, 225,   2,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  80, 254, 254, 254, 254, 254, 247,  70,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  86, 254, 254, 254, 254, 254, 254, 245, 102,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  45, 109,  44,  44,  98, 236, 254, 254, 243,  18],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 254, 254, 254,  98],\n",
       "        [  0,   0,   0,   0,   1,  40,  95,  95,  42,   0,   0,   5, 203, 254, 254, 199],\n",
       "        [  0,   0,   0,   0, 111, 254, 254, 254, 168,   0,   0,   6, 206, 254, 254, 112],\n",
       "        [  0,   0,   0,  59, 253, 254, 254, 254,  84,   0,   0,  97, 254, 254, 254,  57],\n",
       "        [  0,   0,   0, 126, 254, 254, 254, 138,  14,  35, 139, 250, 254, 254, 217,  10],\n",
       "        [  0,   0,   0,  75, 246, 254, 254, 150,  11, 216, 254, 254, 254, 254,  65,   0],\n",
       "        [  0,   0,   0,   0, 118, 247, 254, 254, 209, 249, 254, 254, 243, 110,   2,   0],\n",
       "        [  0,   0,   0,   0,   0,  84, 246, 254, 254, 254, 253, 197,  40,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  26, 189, 254, 175, 114,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(im3)[4:30,4:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f3353",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = [tensor(Image.open(x)) for x in threePaths ]\n",
    "sevens = [tensor(Image.open(x)) for x in sevenPaths ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ae8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131 6265\n"
     ]
    }
   ],
   "source": [
    "print(len(threes), len(sevens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c15f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJZklEQVR4nO2b3W8i1RvHPzMMMECx0CKW0hYobXebdBt3jdXd+NLEC80aL/TGmHjvv+CliYn/iYkmGrN7o8lqXJeLra5Vd91ldWlDW+gLFAsFBhimMF7ozLZs3dfpy+8XPslkwmE688yX55zzPM85FXRdp8tdxKM24LjRFaSDriAddAXpoCtIB9IDvv9/noKE/Rq7HtJBV5AOuoJ00BWkg64gHXQF6eBB0+4j02g02NraYmdnB03TkCQJp9OJKIqI4l39jc+SJGGz2cx2QRAQBOGe9sPCckGy2Swffvgh2WyWdDrN4OAgZ86cweVy0dPTY17X09ODz+cjGo0yPDxstjudThwOB8FgcM/1h4XlgoiiiCzLSJJEvV7nr7/+YmFhAbfbvecFDYEKhQLr6+tmuyzLyLLM6OgowWAQu92O3W7H6XQiSRJ2u/1APcdyQbxeL7Ozs9y5cwdFUdja2mJubu6e64zu09mVbDYbgiAwMTFBJBIhEokwMDDAzMwM4XCYgYEB3G631WabWC6ILMtMTU3h8/mw2WxUq1UKhQKapqGqKuVymUKhYP7yrVYLTdOoVqvUajVzDGk2m5RKJURRNMek4eFhzpw5Y3Ynh8NhtfkID6iYPXIuo+s67XabdrtNq9Uyz41Gg3K5TCaT4aeffsLr9RIIBCiXy1QqFdLpNEtLS+Z9VFVF0zRSqRS5XM4U8P333+e5557jzTff5Omnn370N77LvrmM5R4iCAI2m83sCoZANpvN7PuCIOB0OnG73aiqiqqqDA0NEY/Hzfs0m000TSMej5PP5/n1119ZW1tjcXERURQZHx9H0zT6+/txOp3W2W+1h9z3Zv8+a79ndrYZQhaLRYrFIp988gmffvopNpsNh8PBW2+9xcTEBB988AGDg4OPY87heMh9LRCEPef7oeu66WHNZpNWqwVAq9VCVVWWlpaQJIlGo2Gpjcc2UhUEAVEUqdfr5PN5FEUxv2u329y4cYNvv/2WSqVi6XMP1UMeBl3X0XWdRqNBo9Hg1q1bzM3NsbKyAtydpk+dOkU8Hsfr9Vr6/GMpSLvdJpfLkU6n+eyzz7h48aLZNSRJQpZlzp8/z0svvUQgELD0+cdGkJ2dHVqtFltbW2xubvLLL79w69YtUqkUqqqaOdGLL75INBrl3LlzxGIxy2ORYyOIpmnU63WuXr3KN998w9zcHMlk0px93G43Xq+Xt99+m1dffZVoNIrH47HcjiMTxOgaiqJQLBZZXV1lcXGRa9eukUwmKRQK6LqOy+XC6XTy+uuvc+LECWZmZggGg0jSwZh+ZIIYEeza2hqXL19mbm6Oy5cvs729TblcNq8zItp3332XV155BY/HcyAhu8GhC1Kv1ymVSuRyORYWFrh9+za3b98mlUpRLpdRVXXP9X19fYyOjpoR6e5E8CA4dEHK5bIZQ3z++edUKpU9HtHJ4OAgsViMQCCALMsPFdQ9CYcmiKqq1Ot1/vjjD7788kvTI5rN5n3/bnFxEVVV+eqrrzh9+jSnT5+mr6/PzJcsxwiE/uOwjHK5rN+8eVP/6KOPdFEUdUEQzEMUxQcesVhMf/nll/VEIqEriqLv7Ow8qUn7vvOheYjNZqOnp2dPcUfXdYaHhxkbG2NqaoqTJ0+akej6+jpbW1usrKyQzWYpFossLS1x4cIFMpkMs7OzBAIByz3l0AQRRRG3243L5QLuZrcjIyOcPXuWd955h2effRZRFBEEgVwux9raGolEgps3b/LDDz+QzWb54osvuHr1KqOjo/T29lrebQ4t/Teq8NlslkQisUeQgYEBwuEwfr//H6MEgXq9btZki8Uily5dIplMkkwm2dzc5L333mN6eprz58/T39//OCYdbfovSRKSJBGPx4nFYma74RGds4fL5cLlcuH3+9F1HYfDQSgUIpPJ8Pvvv3PhwgV+++03XnjhhccVZH87LbvTQ2Kk9bs/328qNb4TRdEsQOu6TrFYRJIkNE2z1L4jEeRxYgljsDXEVBQFURQtF+TYFog62d7eZnl5me3t7QN9zv+MIJVKhXw+T71eN9sOImo9Nun/f9FoNFBVlfn5eRKJBLlcDoBgMEg4HEaWZUufd6wF0f8tMJfLZVZWVvjzzz/RNA1BEPD7/QwODmK32y195iML0m63aTQa5gq9cbYawzOuXLlCIpHg2rVr5gBqt9uZnZ1lZmbGjF2s4rEFgX9W6o2pcPfxJBg5haqqbG9v8/PPP/P111+zurpqLngZ8cypU6csXaSCxxCkXC7z8ccfU6vV8Pl8+P1+JicnCYVCnDx50lypf1RxDCE2NjZYWVkhkUhw48YNrl+/TjabpdFoIIoiY2NjjIyM8PzzzxOLxY5eEEVRuHjxIsViEb/fTzgcZnNzk/HxcUKhEB6PZ0/M8CBRjBDeWA/O5/PMz89z5coVvvvuO7P47HA4kGWZkZERc6vEkdZU2+02zWYTRVEQBAFN08jn85RKJVZXV+nt7eXSpUv09vYSjUbx+XwMDQ3h8/no6+u7535GfWRjY4O1tTWazSaNRoNUKkUymWR9fR1N05BlGafTyRtvvMGJEyd47bXXGBoaIhgMWiqEwUMLouu6+Wu53W5kWaZaraKqKqVSCYDr16/j8/mYnJxkYGCAaDRKf38/0Wj0Hk+pVCqUSiUymQzpdJparUatVmN5eZlcLmd6mNfrxev1MjExwblz55icnLR8IN3NQ2e7+r9V8mq1yvfff08mk2F+fp6NjQ1+/PFHcxVfkiSzEOx2u3E6nfu69s7Ojuklu5cpNU2j2WwyPT3NxMQEZ8+eZWpqikgkgt/vR5Zlq6baJ8t2jW0OsiwzPT1NKBRCURQ8Hg8LCwvUajWq1SrtdttM3XeH2cZeEQNjfDEKPA6Hwzx6e3sZHx83lx3i8bi5HHHQPHI9xBhLjE0wiqKwvLxMqVQinU5TKBRIpVJ7Xh5gfX2dO3fumDsPQ6EQkUiEQCDAM888w9jYGLFYzKyquVwuZFnG5XLhcDju2Xplxbvv1/jIs4yxqQ7A4/Hw1FNP4Xa7URSFQCBALpczu9duPB4P9Xrd3N8xPDzMyMgI4XCYSCRizlLGIHpUPHHFzBhsjbPhQZ0YYwNgRrd2u93cj2oUkA7AE/6LfT3kUHcQHTO6/y/zMHQF6aArSAddQTroCtJBV5AOHhSYHezeg2NI10M66ArSQVeQDrqCdNAVpIOuIB38DSUr7zHA9PaPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(threes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_threes = torch.stack(threes).float()\n",
    "stacked_sevens = torch.stack(sevens).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(255.)\n",
      "tensor(0.) tensor(255.)\n"
     ]
    }
   ],
   "source": [
    "print(stacked_threes.min(), stacked_threes.max())\n",
    "print(stacked_threes.min(), stacked_sevens.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc71d2",
   "metadata": {},
   "source": [
    "Image pixel values are between 0 and 255 so we can scale it to be between - and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_threes = torch.stack(threes).float()/255\n",
    "stacked_sevens = torch.stack(sevens).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26289e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6131"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stacked_threes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97234d75",
   "metadata": {},
   "source": [
    "We now have all our images stacked up in a single tensor (can imagine a cube stacked up with 2D images). Each image is 28 x 28, and for threes we have 6131 images. Interestingly the len() of these pytorch tensors are the rank (or dimension), i.e. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(stacked_threes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c2aca",
   "metadata": {},
   "source": [
    "## compute the \"average\" 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff75556",
   "metadata": {},
   "source": [
    "We simply compute the mean over all images, i.e. mean along the 0 axis (the one of length number of images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea312a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJtUlEQVR4nO1b2XLiWhJM7QsChDG22x3h//+qfnKzWVhoX5HmoaNqDufK9jRge2aCiiCEAS0nVUtWlqz0fY+r/dvU776A/za7AiLZFRDJroBIdgVEMv2D7/+fS5Ay9OHVQyS7AiLZFRDJroBI9lFS/RQ7tV1QlME8eFH7dEDkxdPf4udDAMmLVxQFfd8Pfn5JuyggQ4vs+/7o1XUdfy6+l01RFCiKAlX9E9WqqvJn9JJ/fwm7CCDy4ruu423XdTgcDjgcDqiqCm3boq5rtG2LpmlwOBzQti3vr6oqVFWFaZrQNA2WZUHXdViWBU3ToOs6NE3j3xFQZOcCcxYgQ15AIHRdh7Zt0bYtqqpCXdcoigJlWaIoCtR1jTzPUdc1yrLk4+i6DlVVMRqNYFnW0dY0Tdi2DcMwYBgGNE1jEGRgTrWTARHBkD2haRpUVYU8z1EUBcIwRJIk2Gw2CMMQu90OaZoiiiKUZYk8z3E4HNB1HUzThGEY8DwPo9EIi8UCs9kMj4+PmM1mmM/ncF0X4/EYlmUdeY4YYqeCc7aHDAFSliWqqkKSJMjzHLvdDmEYYrPZII5jBEGALMsQRRGyLEOWZRw6dOdnsxk8z0PXdSiKAoqioKoq6LqOw+EAXdfR9z17CYXPUOL9dEDkXCHmiKqqEMcx0jTFdrtFGIZ4fn7Gfr/HZrNBmqYIggBJkiCKIlRVhbIs0bYtDocDn8NxHFiWhYeHB9ze3iIMQ9zc3CDLMtze3qJtW0wmEyiKAsdxjpLvOXZ2UhXzBy1K3LZty+GlaRpM08RoNIKiKNA0jQFpmgZt27Knkaf0fY+6rlHXNYdhlmX8N53DNE2+ji/3EAJiKHfQhdZ1zVVEVVVYlsVxb9s270P7U/WhvNM0DZqmga7rnJgp76iqivl8Dk3TMB6Poes6uq7jkKEbcAowJ4eMaHRiimNd19kTyHNs24Zt27xQApRAITAJkKIokOc5NE07SpbyfqKHXsIuRszoonVdh+M4nOw8z4Pv++wB8j60QAJgv98jjmOuTMRZdP3PpYqe+R4Q31Jl6MQiGADQdR2Tp6ZpOESImYpGn2dZBl3XUVUViqJgPkILo5xD5zEM44ikib87x04CRD45ubVpmnyRXdfBcZyjaiTfTSJvTdPAsiyYpvmPUAFwlBNM02SCRpxlCLxT7SwPES9AVVVehBgKsnuLpZq25BVhGGK/32O/3yNNU+R5zhVLVVUYhnHEXonWEykb6nG+HBDxLhIQcqKTgaBS3Pc9V4+Xlxes12usViu8vLwgiiLs93tesKZpsG0bk8kEvu9jNBrBdV0GhRL6uXYyIDIQiqKg67pBUESeIvKJJEmYwa7Xa2y3WwRBgN1uhyzLkOc5JpMJl2rXdeF5HsbjMRzHOQpRsRv+FkBEUAgEIlJvtfvU4NHdXy6XWK1W7BUESBiGHIau60LTNDiOg8lkwpTecRw4jnPkHd+WQwgA8b28JRAOhwN3tHEcI4oiDo3lcsmhst1usdlsUBQFqqri5EkVi7jNUHW5VIU5GZD/BBSRxRZFgSRJsN1usVqt8OvXL6zXazw/P+P3799YLpeI4xhJkvAxPc+D53kA/lQxSqhi6y+LRnQt59jFRGbxQihcyDuKouCmbrPZYLPZIAgCLJdLBEGANE1R1zULRCLPAHBUiaiBpLZAZqvnMtazc8iQZirS67qukWUZ4jjGer1mQJbLJZbLJZIkQZIkXIYVRTnyAjpWVVUsFbiue9QMUh/zrSHznhFIIg+h5EpyoO/7WCwWGI1GmEwmDKSmaZxEKUwo7KIoQhAE3NRR9yxrr8A3UnfRZJFZJmZEvy3Lgud5uLu7Y3WNTNRLAcAwDHRdhzzPoaoqXl9foWkabm5uoOs6bNvm4367h7w3SlAUhXOB67po2xY/fvxghlkUBbIs49AiI/BkQbrve65UqqoiiiJomgbXdTnviJ5C1/C3dramKr8XL0am2/P5HI7jwHVdFn1kSk+9DVH3OI4ZOGK1qqpiv9/DsixUVcVeJHriqXaWHjK0FUsxjRNc14VhGDBNE3VdYz6fs2fIgJCC9vr6ijiOmXiRStY0DVet0WiEsixhmib3O8SWvyyHvFVVxO+o+pD7ip0pdbhDs5yu61igtm2bQ4tAAoC6rqFpGqv1ojInMmU69t8Cc5aHyG39EDAU3zRzeav5I05BJZd6HgLGMAxOvqJsQFLkECf58hwytDBxS6CQejaUa2QPIbNtm0uvyErl38tgnGt/BYjsGVQd3tM236PW4gLp+LRYyhUkWFOYDc13L8FQyU7OISIAlBxliVAeWL8Finx8keWSQCQn7KF9ZftS1V2Me3FoTQuStVZRURMBot9TcozjmGn+arXCbrfjkSclTmK7NBCn7ve9pwM+DRBZ6xABaZrmKBcQ42zbli9cFKMVRWFQReEoTVNW37MsQ1EURyFD9F7WU7+NqYqA0BCpbVu+8Kqq+Ht5VkPAkFG1oMZts9mwRvL6+ookSVAUBatjJBT5vo/pdMojTzr2uU3eSUlVBoW8g2a0RVFwDiAKL48OyGhARSradrvFbrdDEAQcKsRGiehRBSJuQ99dwlP+ChBZFBJPTqSqLEsEQcC0m7xIfIaDRo9Ex8uyRJqmSNOU5YA8z1GWJfMQz/MwnU5xf3+Pu7s73N3dYTKZwPM82LY9mEc+HRARGPmkYnIkgTiKIvYcAk0UpEV5kYbYSZLw4xF93zMpcxwHnudhMplgMpnAcRxmwEO66qn214CIYFBiI+2TdFAAR3khDEPUdY04jo9yDok8IuOkhOn7PsbjMR4fH+H7Pp6ennB/f4+npydMp1PMZjMGhfb58pAZAoUSpvwiLxCbsd1ux5WE8g6Vb3J3Yqeu68L3ffi+j/l8jsVigdvb26MwuVQiPRkQOinxCHERNL60bRsAMJ1Ooes69vs9DMNAkiQwDIPlRNI66O7SvGU+n2M8HuP+/h6z2Qw/f/7EYrHAzc0NC88iGEPc5ssAkcERgSHdAwCr5VmWQVGOH4Ui8IjIkUfNZjOMRiPMZjP4vs9PDj08PGA8HnPeoFmMqKxdcgyhfNADDH451NOQuEP6Z9M0XCnSNOVHrUg9pypDi6PRpLilZ0rkofZQiT0BjMEdzgJkiLVSmaXRgchPaEu5gzQTEotN02SSRS+x2x16NvUMr7gcIPzlAFED/tn9fjQ7kZO03JO81aOcGSKDO19ktiv/LbblQ9uPjvfW9q3zXtLO8pCP7BIaxScu/vIe8uEZP/FOfpZd/4FIsisgkn0UMv97Pn+mXT1Esisgkl0BkewKiGRXQCS7AiLZvwBtCZqwAvXF1QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_three = stacked_threes.mean(0)\n",
    "show_image(mean_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15365f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_svn = stacked_sevens.mean(0)\n",
    "show_image(mean_svn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57a3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mean_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da84c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(image: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Classify as 28 x 28 image represented as a pytorch tensor\n",
    "    \n",
    "    Returns the digit which has the least mean squared distance from the image. i.e. we decide if itsa seven or a three \n",
    "    by seeing which \"ideal\" digit its closest to\n",
    "    \"\"\"\n",
    "    distance_three = ((image - mean_three)**2).sum().sqrt()\n",
    "    # or, compute this with the pytorch loss function MSE (mean squared error) \n",
    "    distance_three = F.mse_loss(image, mean_three).sqrt()\n",
    "    distance_svn = F.mse_loss(image, mean_svn).sqrt()\n",
    "    return 0 if distance_svn < distance_three else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affced42",
   "metadata": {},
   "source": [
    "## Evaluate our classifier on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142389b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testThrees = [tensor(Image.open(p)) for p in (path/'valid'/'3').ls()]\n",
    "testSvns = [tensor(Image.open(p)) for p in (path/'valid'/'7').ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89089a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = testThrees + testSvns\n",
    "y = tensor(len(testThrees)*[1] + len(testSvns)*[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c366729",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tensor([classify(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41738c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 0, 0])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2286a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9475)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = 1- ((y - y_pred)**2).sum()/len(y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36d221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9161)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision on predicting 3 \n",
    "# i.e. out of all the times we predicted 3, what fraction was correct? \n",
    "precision_3 = torch.logical_and(y_pred == 1,y == y_pred).sum() / (y_pred==1).sum()\n",
    "precision_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b96830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9842)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall on digit 3 \n",
    "# i.e. out of all the true 3 digits, what fraction did we \"hit\"? \n",
    "recall_3 = torch.logical_and(y == 1,y == y_pred).sum() / (y==1).sum()\n",
    "recall_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33820aa9",
   "metadata": {},
   "source": [
    "Our recall is more than 98%, so almost all of the actual 3's we predict correctly. OUr precision on 3 is a bit lower, so that means that we're predicting 3 on some digits that were actually a 7. As you can see below, we are predicting 1085 digits to be a 3 but there are only 1010 true 3s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887de53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1085)\n",
      "tensor(1010)\n"
     ]
    }
   ],
   "source": [
    "print((y_pred==1).sum())\n",
    "print((y==1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762afa1c",
   "metadata": {},
   "source": [
    "Overall pretty damn impressive for such a naive algorithm! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e2b02",
   "metadata": {},
   "source": [
    "## Vectorize that code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23654ac8",
   "metadata": {},
   "source": [
    "I wrote that off the top of my head, and one bad thing I did is use a python loop (list comprehension) to compute all the predictions in batch on the validation set. Lets rather do this in a vectorized manner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_distance(im_a, im_b): return ((im_a - im_b)**2).sum().sqrt()\n",
    "def mnist_distance(a,b): return (a-b).abs().mean((-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_3s = torch.stack([tensor(Image.open(p)) for p in (path/'valid'/'3').ls()]).float()/255\n",
    "valid_7s = torch.stack([tensor(Image.open(p)) for p in (path/'valid'/'7').ls()]).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_3(image): return mnist_distance(image, mean_three) < mnist_distance(image, mean_svn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.stack([*valid_3s, *valid_7s])\n",
    "y_val = tensor(len(valid_3s)*[1] + len(valid_7s)*[0]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8be623",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = is_3(X_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c68e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9514)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (y_pred == y_val).sum() / len(y_val)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f5939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9841)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_3 = torch.logical_and(y_pred == 1,y_val == y_pred).sum() / (y_pred==1).sum()\n",
    "precision_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c000b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1013,   15],\n",
       "       [  84,  926]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7cf2b",
   "metadata": {},
   "source": [
    "## Training a REAL model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19efff4",
   "metadata": {},
   "source": [
    "lets prepare our training data - we'll flatten each image into a single vector (28x28=784) and use 784 weights, one for each pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53edf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = [tensor(Image.open(x)) for x in (path/'train'/'3').ls() ]\n",
    "sevens = [tensor(Image.open(x)) for x in (path/'train'/'7').ls() ]\n",
    "stacked_3s = torch.stack(threes).float()/255\n",
    "stacked_7s = torch.stack(sevens).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35efcc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(stacked_3s.shape)\n",
    "print(stacked_7s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a30d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast each 2d image into a 1d array \n",
    "train_x = torch.cat([stacked_3s, stacked_7s]).view(-1,28*28)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b1619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in python list1 + list2 concatenates the contents of the two lists\n",
    "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e10669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = w*x + b\n",
    "weights = init_params(28*28)\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf23d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(X):\n",
    "    return X@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0036221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2062646410148c61e60dd5b8369538da1a9b26d21d601cb9bd1ea173ddc25c8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
