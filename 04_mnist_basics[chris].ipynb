{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3b16b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "! pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46fd7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8d9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058ca85",
   "metadata": {},
   "source": [
    "# Read input data: digit images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176c61b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/mchristos/.fastai/data/mnist_sample/labels.csv'),Path('/home/mchristos/.fastai/data/mnist_sample/valid'),Path('/home/mchristos/.fastai/data/mnist_sample/train')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad907c44",
   "metadata": {},
   "source": [
    "We have a training and validation set of mnist digits 3 and 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7afda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "threePaths = (path/'train'/'3').ls()\n",
    "sevenPaths = (path/'train'/'7').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7855ae",
   "metadata": {},
   "source": [
    "Run below to see a random three or seven image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e842a7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nGNgGGSAEc5Sj9Nf+YrhyFesyhTOPa14/e+zBU5zMt7+3cCLU9b0798QnJLsp45J4JTM+puAU07zBYokE4pknThOfQzT//x/zolL56M//0V7cOtlkPg1BY/s3LO4HMTAwLANl50MDAysKu24TZX8W4hPEo+dybh0qZ3nsnz3dx52SYvPzz//zefAobVy389KVtzuoRYAAF+bL6ESaJgvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7FE3350EFDC0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomIndex = np.random.randint(len(threePaths))\n",
    "im3=Image.open(threePaths[randomIndex])\n",
    "im3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451591a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/ElEQVR4nGNgGNpA3Lrg37sErFLs+ef//f379wSGhKcBQ8Hxv39fr3v69zqalPC63y3zfz085s8vdfPvZDSrzv/9+/ffIiEGBoZZf996ocip3fz3799zHwYGBgbVk/8OiKOYuerf3/cNEgwMDAwMu/6+10eWkzz1998NMwhb/sb3GBRDm/7+3SEJZU/7d1oOWa7h9d+NulC2/Pt/lchymf/+nYe6gNX6/b9pKIYe+bUZZqbV37/n1VD96GQGY83++9aJAQeo/vUhlxWHnM+Nv8s5cMhJHvp/Rg6HHEPj3589uOTMP/zdroVDjuPs37+zuHBJLv97XReHHMkAAIWRZi+H+gQVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7FE3F0390B50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomIndex = np.random.randint(len(sevenPaths))\n",
    "im7=Image.open(sevenPaths[randomIndex])\n",
    "im7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a2fdc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  39, 133, 180,  93,  71,  71,  11,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  32, 206, 229, 120, 235, 254, 243,  56,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 104, 237, 253, 176,  13,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  53, 253, 253,  84,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 202, 198,  24,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 106, 253,  96,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  41, 232, 253,  96,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 167, 255, 253,  96,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0, 151, 252, 255, 231,   9,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0, 121, 248, 254, 252, 149,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 250, 148,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 157, 205,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  83, 205,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5, 119,  84,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  25, 253, 113,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  25, 253, 205,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 124, 253, 205,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  38, 207,  10,  57, 238, 253, 158,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  56, 243, 231, 243, 253, 111,   8,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0, 121, 190, 249, 121,   5,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(im3)[4:30,4:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f3353",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a3a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = [tensor(Image.open(x)) for x in threePaths ]\n",
    "sevens = [tensor(Image.open(x)) for x in sevenPaths ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c3ae8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131 6265\n"
     ]
    }
   ],
   "source": [
    "print(len(threes), len(sevens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c15f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(threes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7403a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_threes = torch.stack(threes).float()\n",
    "stacked_sevens = torch.stack(sevens).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a490dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.)\n",
      "tensor(0.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# confirm that pixel values are in the range 0-255 (i.e. 256 range)\n",
    "print(stacked_threes.min(), stacked_threes.max())\n",
    "print(stacked_threes.min(), stacked_sevens.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc71d2",
   "metadata": {},
   "source": [
    "Image pixel values are between 0 and 255 so we can scale it to be between - and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe52358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_threes = torch.stack(threes).float()/255\n",
    "stacked_sevens = torch.stack(sevens).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "509f851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e26289e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6131"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stacked_threes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97234d75",
   "metadata": {},
   "source": [
    "We now have all our images stacked up in a single tensor (can imagine a cube stacked up with 2D images). Each image is 28 x 28, and for threes we have 6131 images. Interestingly the len() of these pytorch tensors are the rank (or dimension), i.e. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde0457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(stacked_threes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c2aca",
   "metadata": {},
   "source": [
    "## compute the \"average\" 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff75556",
   "metadata": {},
   "source": [
    "We simply compute the mean over all images, i.e. mean along the 0 axis (the one of length number of images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cea312a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_three = stacked_threes.mean(0)\n",
    "show_image(mean_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e15365f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAI6klEQVR4nO1baVPiTBc9ZF/IhoOWOjUf5v//KgelNEZICCErvB+eutemjaNC8N04VanG7H1y99uOdrsdzniF8u9+gf80nAmRcCZEwpkQCWdCJGgfHP9fdkGjvp1nCZFwJkTCmRAJZ0IknAmRcCZEwpkQCWdCJJwJkfBRpHoQPlNjkc8ZjXoDxzf47HmHYhBCxMnR74/Gz0IkYDQa8d/y2Hf+ITiKEHmS2+2Wx91uxxv9TcfFY+L1BHGy9FtRFIxGo96RjsvXH4KDCBEnIk6aJt51HbbbLbquQ9d1aNuWR9pP58n3kUET1zQNqqrCMAyoqgrTNKGqKjRNg6IoexvhEGK+TIj44kQCEdA0DZqmQVmWaJoGm80GdV1jvV6jrmvkeY6qqrDZbNA0Deq6Rtu2TFSfJKmqCkVR4LouTNPExcUFXNdFFEVwHAe+78M0TViWxQSNRiOoqordbvdlUr5EiCwZ9KVJAoqiQNM0WK/XKMsSWZahKAokSYLNZoMsy1CWJRPVNA1fS6SKKgaACRmPxzBNE9PpFJ7n4devX/B9H5qmYbfbMRHb7RaKohxExpcI6VMPmgxNcLVaoSgKxHGMNE0xn8+RZRniOEae51gsFlitVlgulywpoiqJ6kQbffUgCOB5Hn7//o0wDJGmKS4vLwEAQRBA0/6ZCqnYyQnpI0ckhlSFJCHLMqRpymOe50iSBOv1GqvVCm3boq7rvfuJIHLatkVVVRiNRmiaBlEUQVEU5HkO13VZ0kjCjsWXVUYkous6NE2DqqpQliXyPEeapojjGMvlEnEcI8sy3N/fY7VaIUkS1HWNsiyZAEVR2DCS1xCfQXamrmvoug7TNFHXNS4vL2EYBvI8h23bLK3vGeeTEPI3kIskEdd1HbquwzAM+L4PVVUBgL+6fA55ETKyy+US6/UaWZZhvV7zMwDskSkSSb9Fd/1VfIqQjxgXyaAJmqYJ27aZhPF4jDAMoaoqu03HcfhcXdehqip7qiRJkGUZ7u7u8PT0xAabDCdBdrnHkPFpQshIyQQoisISsd1uYZomuq5DEARQFAV1XcO2bei6ziqg6zosy4Jt23BdF5ZlwbIslpDNZoOyLHl/nucoioLVgQik0bIsjk2+jRCRiD5CDMMAALiuC1VV0XUdTNOEoiioqgphGLKtoNjB8zy4rssTo3sSIZ7nYTabIc9zrNdrVFWFtm1hWRYcx4Ft27Btm4kjCRNV5tu8jKizAKDrOn89ALBtm41j13WoqgqapsE0Tbiuy5LhOA50XedYYrfb7UWmwD/qRt5IVVU4jgPXdTEejxEEAUuIaJiPwacJER+kKAoHQPTypNuqqrL6ULS42+1YVSzLYsmwLIuJJZUiIk3TZEKqquKYxHEcOI4Dz/M4WqUoVbQjJydEJIaCHjHxAv6RFABMhghR98muEJF0Pbnbuq6RZRkHcuRlDMPAeDyG7/sIw5CjV13X3+Qxh+JglQFeJYV0V9d1Jqxt2z2dFr0P6TtNgK6h2KYsS6RpisVigcViwUEYqVwQBIii6A0hx7rcLxMiehvRsJLEkOEkkogQ2i8TQRCDvDzP8fz8jMfHRzw/PyNNU9R1zaG77/sIggCu68K27T3SAey938mTO3oQPVj0OuRxSBrETJU2MVUX70PGl/Kh+XyOp6cnzGYzpGmKpmmgaRo8z4PneQjDkCWGpGMoHBypytICvNoSsh80igUdoL+EUBQF0jTF/f097u7uEMcx4jhG0zRQVRVhGGIymfBIrvZYFZFxVOjeV84Tv5ZMmLyfJKNtW2w2G86QHx4eMJ/PkaYpezFys7TJrnYoUg42qrItod8A9ryGDLmMQMlekiT48+cPZrMZ5vM5FosFmqZhb3J1dYXLy0tMp1OMx2OOP0TJG4KUoyVEtCVkYGkTiZOLS2RI67pm6Xh4eEAcx3h4eECe5+i6DoZhIIoihGGIi4sLNqhiZErvMgSOtiF9Ve/tdvsm/wH2SaGSY1EUeHl5wWw2w2w2w8vLC6uK53m4vb3Fz58/cX19jZubGwRBwEmhHIx9u9v9G+RIVm5NyG6RCktUR0mSBEmSYLFYoCxLqKoK27bx48cPXFxcYDKZYDKZwHEcmKb5xn70kfBtucxnH0xSIhdtKHCjuuv9/T3HHGVZQtM0RFGEIAhwfX2N29tb3NzcIIoi2LbNkXBfyn+sCg1iQ+S/33sZUWWoClYUBZbLJfI8R5Zl7GZ938d0OkUURZhMJvB9H7Zt73mX98g4BkdLSB8phPdUheqvaZri6ekJSZJwi4LynNvbW0ynU5YQMqaidPTZjW/Ldv8GedLv7SPVIemgEiGRQV5FzGajKILneRyIDW1EZQza7O7zLMCrVxGN6HK5xOPjI6vLdruF4zgIggC2bePq6gpXV1fchxErY3LkKz7/WJx8OURfy4J6Mnmec08HANdIKGchcvq8CtAfFB6LQSVEVg+5b0M9mcVigefnZywWC2w2G4xGI1aJyWSCKIrY1ZLdoJrr0KG6jJOtD/lbD2ez2aAoCpRlia7rOF/RdZ2Lz5TeExGniEr7cJL1IWJoTipSFAVWqxWyLMPLywvyPOe2AnkWUUJEdZELQKfE4CrTJx3Ua6mqipO5uq65qKxpGtsPalGIlfTvIgMYgJC+tSLUZyXpoJ7ver1mQ0pVNbHOats2wjCE7/uwLOvDmOMUGNyG9NkOMqo00nIHsTBMTScav8uIyhhsSdV7RpQWxtBGrQbTNN8siKEikOhqRYP6X6EyIvoW1IgrgyiUp8YUqYSqqiwdhmFwi2KIPstXMWhy99451AR3XZcnKa8CIBsitiffa1mI49A4SRwC7Lc7adLU6gReWw+apnGb0zAM3khy/lbzOAUpow++8KdWnvTZEHHJFdkSMqxt2+6pEEkRkUNumEj5qDJ2IDG9Fw1eMaNqmdinESdMC+xEQoDXxXWiIZXvcYp0/808hpAQPrknYhX391XPel+qJ4EbqiImPqZ355CE8EXvlADeO953ft/EB5aKgwj5v8P530MknAmRcCZEwpkQCWdCJJwJkfAv6ObhbeIGuNEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_svn = stacked_sevens.mean(0)\n",
    "show_image(mean_svn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec57a3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mean_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da84c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(image: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Classify as 28 x 28 image represented as a pytorch tensor\n",
    "    \n",
    "    Returns the digit which has the least mean squared distance from the image. i.e. we decide if itsa seven or a three \n",
    "    by seeing which \"ideal\" digit its closest to\n",
    "    \"\"\"\n",
    "    distance_three = ((image - mean_three)**2).sum().sqrt()\n",
    "    # or, compute this with the pytorch loss function MSE (mean squared error) \n",
    "    distance_three = F.mse_loss(image, mean_three).sqrt()\n",
    "    distance_svn = F.mse_loss(image, mean_svn).sqrt()\n",
    "    return 0 if distance_svn < distance_three else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affced42",
   "metadata": {},
   "source": [
    "## Evaluate our classifier on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "142389b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testThrees = [tensor(Image.open(p)) for p in (path/'valid'/'3').ls()]\n",
    "testSvns = [tensor(Image.open(p)) for p in (path/'valid'/'7').ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89089a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = testThrees + testSvns\n",
    "y = tensor(len(testThrees)*[1] + len(testSvns)*[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c366729",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tensor([classify(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e41738c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2286a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9475)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = 1- ((y - y_pred)**2).sum()/len(y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e36d221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9161)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision on predicting 3 \n",
    "# i.e. out of all the times we predicted 3, what fraction was correct? \n",
    "precision_3 = torch.logical_and(y_pred == 1,y == y_pred).sum() / (y_pred==1).sum()\n",
    "precision_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b96830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9842)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall on digit 3 \n",
    "# i.e. out of all the true 3 digits, what fraction did we \"hit\"? \n",
    "recall_3 = torch.logical_and(y == 1,y == y_pred).sum() / (y==1).sum()\n",
    "recall_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33820aa9",
   "metadata": {},
   "source": [
    "Our recall is more than 98%, so almost all of the actual 3's we predict correctly. OUr precision on 3 is a bit lower, so that means that we're predicting 3 on some digits that were actually a 7. As you can see below, we are predicting 1085 digits to be a 3 but there are only 1010 true 3s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "887de53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1085)\n",
      "tensor(1010)\n"
     ]
    }
   ],
   "source": [
    "print((y_pred==1).sum())\n",
    "print((y==1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762afa1c",
   "metadata": {},
   "source": [
    "Overall pretty damn impressive for such a naive algorithm! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e2b02",
   "metadata": {},
   "source": [
    "## Vectorize that code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23654ac8",
   "metadata": {},
   "source": [
    "I wrote that off the top of my head, and one bad thing I did is use a python loop (list comprehension) to compute all the predictions in batch on the validation set. Lets rather do this in a vectorized manner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "29fc09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 distanec \n",
    "def mnist_distance(im_a, im_b): return ((im_a - im_b)**2).sum().sqrt()\n",
    "# l1 distance \n",
    "def mnist_distance(im_a,im_b): return (im_a-im_b).abs().mean((-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74ae22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_3s = torch.stack([tensor(Image.open(p)) for p in (path/'valid'/'3').ls()]).float()/255\n",
    "valid_7s = torch.stack([tensor(Image.open(p)) for p in (path/'valid'/'7').ls()]).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a6e06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_3(image): return mnist_distance(image, mean_three) < mnist_distance(image, mean_svn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7623256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.stack([*valid_3s, *valid_7s])\n",
    "y_val = tensor(len(valid_3s)*[1] + len(valid_7s)*[0]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e8be623",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = is_3(X_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb0c68e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9514)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (y_pred == y_val).sum() / len(y_val)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "412f5939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9841)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_3 = torch.logical_and(y_pred == 1,y_val == y_pred).sum() / (y_pred==1).sum()\n",
    "precision_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c000b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1013,   15],\n",
       "       [  84,  926]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7cf2b",
   "metadata": {},
   "source": [
    "## Training a REAL model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19efff4",
   "metadata": {},
   "source": [
    "lets prepare our training data - we'll flatten each image into a single vector (28x28=784) and use 784 weights, one for each pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a53edf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = [tensor(Image.open(x)) for x in (path/'train'/'3').ls() ]\n",
    "sevens = [tensor(Image.open(x)) for x in (path/'train'/'7').ls() ]\n",
    "stacked_3s = torch.stack(threes).float()/255\n",
    "stacked_7s = torch.stack(sevens).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "35efcc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(stacked_3s.shape)\n",
    "print(stacked_7s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e31a30d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast each 2d image into a 1d array \n",
    "train_x = torch.cat([stacked_3s, stacked_7s]).view(-1,28*28)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d94b1619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in python list1 + list2 concatenates the contents of the two lists\n",
    "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2fbc8901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a dataset of (image, label) tuples \n",
    "dset_train = list(zip(train_x, train_y))\n",
    "x,y = dset_train[0]\n",
    "x.shape, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fb9dfd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2038, 784]), torch.Size([2038, 1]))"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3s = [tensor(Image.open(x)) for x in (path/'valid'/'3').ls()]\n",
    "valid_7s = [tensor(Image.open(x)) for x in (path/'valid'/'7').ls()]\n",
    "valid_x = torch.cat ([\n",
    "    torch.stack(valid_3s).float()/255,\n",
    "    torch.stack(valid_7s).float()/255\n",
    "]).view(-1,28*28)\n",
    "valid_y = tensor([1]*len(valid_3s) + [0]*len(valid_7s)).unsqueeze(1)\n",
    "valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3396d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_valid = list(zip(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "a7b2f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly from a normal distribution around 0 with std=1\n",
    "# remember to mark the parameters as requiring grad, since we will be updating them based on gradients! \n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82db64f",
   "metadata": {},
   "source": [
    "Initialize a model by defining a set of 784 weights (that's 28*28, i.e. a weight for every pixel) plus a bias. We compute the prediction by taking the dot product of an image with the weights vector, and adding the bias (a number). \n",
    "\n",
    "Since weights are pulled from a normal distribution around zero, they are equally likely to push the final prediction to be a positive or a negative number. So we can arbitrarily decide that a positive number means a 3, and a negative number means 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f3e10669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly initialize weights and bias \n",
    "# y = w*x + b\n",
    "weights = init_params([28*28,1])\n",
    "bias = init_params(1)\n",
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "447ce1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.8456], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute a prediction \n",
    "# (+) => 3 , (-) => 7\n",
    "(train_x[0]*weights.T).sum() + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "cd430027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.8456], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or do the same with matrix multiplication \n",
    "train_x[0]@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8acf23d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.]]),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         ...,\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compute predictions on the whole batch at once, we can use matrix multiplication\n",
    "def linear1(X): return X@weights + bias\n",
    "\n",
    "preds = linear1(train_x)\n",
    "predicted_y = (preds > 0).float()\n",
    "predicted_y, train_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e990c3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4184)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects = predicted_y == train_y\n",
    "accuracy = corrects.float().sum()/len(corrects)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd427c7",
   "metadata": {},
   "source": [
    "Looks like our accuracy is close to 0.5, i.e. pretty much random, and that's exactly what we would expect for randomly initialized weights\n",
    "\n",
    "Now we need to decide on a loss function and start tuning those weights! First we'll use the sigmoid function to make sure all predictions are between 0 and 1 rather than from [-inf,+inf]. Then we can easily get a distance between a predition (say it's 0.6) and the correct answer, either 0 (3) or 1 (7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "6bad6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f352f379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(sigmoid, title=\"Sigmoid\", min=-4,max=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6020ea54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0630e-03],\n",
       "        [8.5616e-04],\n",
       "        [9.6794e-01],\n",
       "        ...,\n",
       "        [9.1929e-01],\n",
       "        [1.4317e-03],\n",
       "        [6.7517e-01]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3e6536b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    \"Computes the loss for a whole batch of predictions. We take the mean of the loss for each prediction\"\n",
    "    sigpreds = predictions.sigmoid()\n",
    "    return (targets - sigpreds).abs().mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "79d02d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5776, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_loss(preds, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e9eba",
   "metadata": {},
   "source": [
    "## DataLoader and mini-batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7d82aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 3, 6, 1, 5]),\n",
       " tensor([10,  8, 12,  9,  4]),\n",
       " tensor([13,  7,  2, 11, 14])]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create 15 data points \n",
    "data = range(15)\n",
    "dl = DataLoader(data, batch_size=5, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "315c5e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 12396)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usually you feed a dataloader a dataset of (x,y) tuples where y is the label and x the input. \n",
    "dl_train = DataLoader(dset_train, batch_size=256)\n",
    "len(dl_train), len(dset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "3894c2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12396 - 48*256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db6c6e",
   "metadata": {},
   "source": [
    "We have 49 batches of size 256 - that makes space for 49x256=12544 data points, i.e. enough space for our dataset (the last batch is smaller, with 12396 - 48x256 = 108 data points - check for yourself by getting the last batch in the data loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "8f62e926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is our first batch of data: \n",
    "x_batch, y_batch = list(dl_train)[0]\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "add6c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another data loader for the validation set \n",
    "dl_valid = DataLoader(dset_valid,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "dd307d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([246, 784])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=list(dl_valid)[-1]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "59c939ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20d12f",
   "metadata": {},
   "source": [
    "## Ok let's try training something using gradients!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d53aebda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually try with a batch size of 4  \n",
    "batch = train_x[:4]\n",
    "batch.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "06b0d534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.8456],\n",
       "        [-7.0622],\n",
       "        [ 3.4075],\n",
       "        [-5.1138]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(batch)\n",
    "preds\n",
    "# remember that sigmoid values are bundled into the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "230ad905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7560, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(preds, train_y[:4])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "523da334",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "91fc2118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0017), tensor([-0.0097]))"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad.mean(), bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "83f8fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb,yb,model):\n",
    "    \"\"\"\n",
    "    Calculate the gradient on all the parameters implicit in the model. \n",
    "    This modifies the gradient attributes on the relevant parameter objects, and these can be used \n",
    "    to update the params \n",
    "    xb: batch of inputs \n",
    "    yb: batch of labels \n",
    "    model: function that computes (batch of) predictions relative to current param values \n",
    "    \"\"\"\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    # modified the gradient attributes of the weights, which are implicit in the \"model\" definition\n",
    "    # , in our case linear1\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f64644a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0017), tensor([-0.0097]))"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to zero the gradients before recomputing, otherwise it adds to the gradients\n",
    "# that are currently stored for the weights\n",
    "#  \n",
    "# note: methods ending in _ in pytorch mean they modify the object (\"in-place\" operation)\n",
    "weights.grad.zero_()\n",
    "bias.grad.zero_()\n",
    "calc_grad(train_x[:4], train_y[:4], linear1)\n",
    "weights.grad.mean(), bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b3f94739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    \"\"\"\n",
    "    model: \n",
    "        function that computes the batch of outputs from a batch of inputs. Model weights \n",
    "        are implicit in this function\n",
    "    lr: \n",
    "        learning rate \n",
    "    params: \n",
    "        Each of the (sets of) parameters that update based on gradient. \n",
    "        In our case, they are \"weights\" and \"bias\"  \n",
    "    \"\"\"\n",
    "    for xb, yb in dl_train:\n",
    "        calc_grad(xb,yb,model)\n",
    "        for p in params:\n",
    "            # update all the weights / bias\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "74a28f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(y_preds,y_batch):\n",
    "    preds = y_preds.sigmoid()\n",
    "    correct = (preds > 0.5) == y_batch\n",
    "    # mean of array of 0/1 values is the same as the fraction of values that are 1\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "7e6cfb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2500)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(linear1(train_x[:4]), train_y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "730f9922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4660862684249878"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function that tells how well our current model is doing on our validation set \n",
    "def validate_model(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in dl_valid]\n",
    "    return torch.stack(accs).mean().item()\n",
    "validate_model(linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "21b59c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.656150758266449"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right now we're doing pretty bad! let's see if training for one epoch helps\n",
    "train_epoch(linear1, lr=1., params=[weights, bias])\n",
    "validate_model(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637d75a",
   "metadata": {},
   "source": [
    "It definitely got better! let's try training for a few more epochs in a loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "3fa84a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6517959237098694\n",
      "0.8329482674598694\n",
      "0.8973815441131592\n",
      "0.931541383266449\n",
      "0.9437285661697388\n",
      "0.9505645036697388\n",
      "0.9554473161697388\n",
      "0.9598418474197388\n",
      "0.9632995128631592\n",
      "0.9642760753631592\n",
      "0.9652526378631592\n",
      "0.9657409191131592\n",
      "0.9672057628631592\n",
      "0.9676940441131592\n",
      "0.9686706066131592\n",
      "0.9696471691131592\n",
      "0.9701354503631592\n",
      "0.9706237316131592\n",
      "0.9706237316131592\n",
      "0.9711120128631592\n"
     ]
    }
   ],
   "source": [
    "# let's train now for a few more epochs! I'll re-initialize weights just to see the uptrend\n",
    "weights = init_params([28*28,1])\n",
    "bias = init_params(1)\n",
    "for i in range(20):\n",
    "    train_epoch(linear1, lr=1., params=[weights, bias])\n",
    "    acc = validate_model(linear1)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "970b0b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1747)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "13a4cf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AxesSubplot:>, tensor([0]))"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAHQ0lEQVR4nO2bXU8TWRiAn+mUDqUUsUURClIoDSghfIgfaATJYmKiiYlE44W3cu3P8GeYeKdXamKC4YIETQT5SoACkQ8htEBbLVPL0A7tzF64dNcuLLJbWrLpc9fOpO/LM++8Z86Zg6DrOjn+xJDtBI4bOSEp5ISkkBOSQk5ICsYDjv+fhyBhry9zFZJCTkgKOSEp5ISkkBOSQk5ICjkhKeSEpJATkkJOSAoHPbofmp2dHRRFYXt7m2/fviGKIpIkUVhYiNVqRRRFjMa0h00bac9sa2uL9+/fMzIywrNnz7DZbNTV1XHt2jXu3buH1WrFarWmO2zaOJIK8fl8+P1+Njc3UVUVURSpra1FURQKCgrSHTKtpF3I9vY2U1NTLC0toSgKkUgEv9+Py+UiGAxiNpspLi5Od9i0kXYhFouFy5cvI0kS09PTRCIRZFlmbW2N0dFRIpEI8Xic/Px8zGbzgb+n6zq6riPLMtvb2/ueJ0kSJpOJU6dOYTKZ/nX+wgGr7odeD0kkEsRiMaanp3n69CkLCwtMTk5is9moqqqipqaG6upq3G43jY2NP4LoOoKw5/IEsVgMVVXp7+9ndnZ27z9CEHA6nZSWlvL48WPsdvuvpLpnwLRXiCAIGI1G7HY7bW1tSJLE1NQU0WiUQCCApmnEYjFCoRA+n4+/XpC9pOzs7KCqKnNzcywsLOwZ02g0UlZWlpb80y7EYDBgMpkoLS2lp6cHs9nMy5cvk0Ox1+tlcnIS+FPAP1XIX4XtV815eXm0trZiNBr3/Z1f5cgeCHarpL29nSdPnhAIBJifnycQCLCxsfHTuZqmYTDs/YyoaRq6rqOqKvF4/KdjBoOBs2fPUlZWRnt7O42NjUiS9J/yTnsPSSWRSBCPx1leXubVq1fMzs4yMDDwtyu/35XVNA1N0wiFQiiK8tMxURS5f/8+9fX19Pb2cvr06cNUyJ4nHrkQXddJJBKEw2EWFxcJBAI/9YKDeoiiKESjUd68ecPY2FjyPKfTSXl5OY8ePaKpqYmGhgYKCwsPk1pmmurfov7RZG02GzabDdi/F+xFNBolFovh9/sZHx8Hftwqly5dor6+njt37lBeXp62fLMyqThM4wsGg6ysrOD3+wFwuVxUVFRw48YNmpqa0j4NOL6zrD9YWlri7du3fP78GUEQaG1t5fz589y6dQuHw7FvM/63HFsh379/R5ZlPn78yKdPn9jY2EAQBCorK7lw4QKFhYUIgvCfh9lUjq2QYDDIzMwMQ0NDDAwMAD+G8pqaGhoaGjCbzWmvDjiGQnab6OjoKO/evWNmZgZBEGhubsbtdtPc3IzNZjuyNZVjJyQWixEIBPjw4QMvXrxAVVUMBgPNzc1cuXIFp9N5pOspx07IysoK/f39zM3NoaoqJ06coKioiPb2drq6uigqKjrS+MdOyNLSUvJWUVWV4uJinE4nLS0t1NTUHHn8rAvRdR1N0wiHwwSDQSYmJlhcXEQQBMrLy7l79y6dnZ1UVlZmJJ+sr7prmkYikUjK2J0AGgwGSktL6ejo4Pr16xlbZct6hXi9XoaHhxkfH2dqaoqZmRmi0Sjd3d3cvHkzOYMVRTEj+WRViK7rrK6u8vr1ayYmJvB4PMCPuUpDQwOdnZ1YrVby8vIyllPWhIRCIRYXF+nr62NwcBBZltF1nY6ODlpaWrh69WrGZUCWhQwNDeHxePjy5QuCIGAwGHC5XHR1dVFeXk5+fn7G88q4kEQiQSKRYGVlhb6+vuST6JkzZ3A4HLS1tdHS0pK1VxUZF6JpGvF4HJ/Px8jICFtbWwiCgN1up7a2Frfbjd1uz/itskvGhYTDYTweD2NjY8iyjNFopLi4mNu3b/Pw4UMqKiowmUxHMnH7FTIeNRKJMDMzw/r6OtFoFIDCwkLcbjd1dXUUFxcjimLap/W/SsYrZHNzk+HhYebn5wEoKSmhsbGRqqqqrFbGLhkVous6iqLg9XqRZRlRFCkpKcHhcCS3SmSbjAlRVRVFUVhdXWV5eZn8/HwuXrzIgwcP6OnpOTYvwDMmJB6PI8sygUCASCSC1WrF4XBw7tw5HA5HptI4kCMXsjub9Xq9PH/+nMnJSb5+/YrL5cLtdnPy5MmjTuFQZESIrutsbm7i8XhYWFggFoshSRIlJSVIkoSmaclzdxeO/7ejzO7bfq/Xy+DgYPJ15NmzZ/ntt9+wWq2EQiG2trZQFAW73U5RURFGozErTTYjQnZ2dpBlmVAolKwGURQRRZHt7W2i0SiyLBMOh5EkCYvFkrUR58iFqKqKz+djfX39p1eYfX19rK2toaoqsVgMURQxGAz09vbS3d2dtZ2KGYmq6zoWi4Xq6moSiQQARUVFhMPhZIXk5+dTUFDwjzsBMkHGtkOoqsrW1laySgRBQBTF5ChkMBgQBAGLxYIkScnPR0h2tkMcY3L/c/cr5ISkcFBTzV53yxK5CkkhJySFnJAUckJSyAlJISckhd8Byh4NoCF7B8AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I want to see it just to be sure \n",
    "rand_index = torch.randint(0,len(dset_valid),[1])[0]\n",
    "sample_x, sample_y = dset_valid[rand_index]\n",
    "show_image(sample_x.reshape([28,28])), sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "e530735c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([0]))"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 => 3, 0 => 7\n",
    "round(float(linear1(sample_x).sigmoid()[0])), sample_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6570ee4",
   "metadata": {},
   "source": [
    "Sure seems to get the correct answer on random validation samples! We get 97% accuracy after 20 epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316760f",
   "metadata": {},
   "source": [
    "## Doing training the fastai way \n",
    "### (using an Optimizer)\n",
    "\n",
    "First off, we use pytorch's built-in linear function instead of manually defining it. This comes with a set of weights and bias built in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "0d9b1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "ffe2437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b=linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "75749962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definte our own Optimizer class \n",
    "\n",
    "class BasicOptimizer:\n",
    "    def __init__(self, params, lr): \n",
    "        self.params, self.lr = list(params), lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.data -= p.grad.data * self.lr\n",
    "    \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "1fbfa6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer class\n",
    "opt = BasicOptimizer(linear_model.parameters(), lr=1.)\n",
    "# training loop can now be written like this: \n",
    "def train_epoch(model, optimizer):\n",
    "    for xb, yb in dl_train:\n",
    "        calc_grad(xb,yb,model)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "72802f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, n_epochs):\n",
    "    for i in range(n_epochs):\n",
    "        train_epoch(model, optimizer)\n",
    "        print(validate_model(model), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "8a0aa99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4931640625 0.7128707766532898 0.8515426516532898 0.9130660891532898 0.9360153079032898 0.9516403079032898 0.9599410891532898 0.9648239016532898 0.9677535891532898 0.9687301516532898 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = BasicOptimizer(linear_model.parameters(), lr=1.)\n",
    "train_model(linear_model, opt, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb7d5e",
   "metadata": {},
   "source": [
    "We wrote the optimizer class ourselves in this intance but, you guessed it, fastai provides built-in optimizers like this. For example, _stochastic_ gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "036e2527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4931640625 0.8198043704032898 0.8329879641532898 0.9072067141532898 0.9340621829032898 0.9496871829032898 0.9579879641532898 0.9653121829032898 0.9672653079032898 0.9687301516532898 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "optimizer = SGD(linear_model.parameters(), lr=1.)\n",
    "train_model(linear_model, optimizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85753011",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
