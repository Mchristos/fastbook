{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of being trained to predict the next word in a sequence, as is the case for RNN models like LSTM, transformers are trained to predict which words are left out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. How do you turn a model that predicts the next word to a model that does classification? \n",
    "\n",
    "A. The layers in the deep model end up learning normally generalizable features of text. The final layer of the model if the one responsible for saying which for it is. When turning it into a classifier, we delete that last layer and replace it with a new layer with randomly initialized weights, and the right dimensions such that we get e.g. a 2-dimensionsal vector out if its a binary classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy: classification problems in NLP are a good start if you're wanting to solve some problems right away"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
