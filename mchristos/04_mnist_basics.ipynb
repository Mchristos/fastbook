{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ba3b16b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "! pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "46fd7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bd8d9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058ca85",
   "metadata": {},
   "source": [
    "# Read input data: digit images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "176c61b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/mchristos/.fastai/data/mnist_sample/labels.csv'),Path('/home/mchristos/.fastai/data/mnist_sample/valid'),Path('/home/mchristos/.fastai/data/mnist_sample/train')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad907c44",
   "metadata": {},
   "source": [
    "We have a training and validation set of mnist digits 3 and 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cf7afda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "threePaths = (path/'train'/'3').ls()\n",
    "sevenPaths = (path/'train'/'7').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7855ae",
   "metadata": {},
   "source": [
    "Run below to see a random three or seven image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e842a7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5klEQVR4nGNgGGSAEUJVxq0+zsDA8OwiNslj5gyM/xkYfn09fevQWnRJISEF69VS7CZxCgxX9LFaIZB4/N+nfnmsciLP/v5diV2KgeHMv3///v+7UIswlhEhuVPxzf4X//1UJCdWYNHKC7F54ucsHGYzMDCI9H9Uxi2r/70etyTDm50Qmgmb5H8e3JLczLtwm+r50xRVQK/UCc5etQxN9fJ/22DMqa900SQb/v5s5mBgYGDgaPnxkhdNkmvev78XLVRVVZf8PW+F6YzTv//+/fvv79+v5dgcGb7+799/J6dq4vYGlQAAxtxOUm5P7H4AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0AC9378A00>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomIndex = np.random.randint(len(threePaths))\n",
    "im3=Image.open(threePaths[randomIndex])\n",
    "im3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "451591a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAu0lEQVR4nGNgGH6AXUhISEjIrOPLv////tmiSHG47v2LBFZBRFkgVG0FVNXlH4dvT4dpgUqqMTA8us7AsO7uqa8McgxokqW7T755SoTLWDf/faSDS7Lu7x8vnDqP/N6NUy76zwOccuJv/h7BJSdx7O8FSVz6jv39K4FL4/K/f5MZsUsxZf/5lYRDjjH7799FuMzM+vv3gCgOuZxff69Y4pCTe/X3lQi6I2AMa2GGaW9w2Rj59wAHLjmsAAAa8ksHf9hzBgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F0AD25B6400>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomIndex = np.random.randint(len(sevenPaths))\n",
    "im7=Image.open(sevenPaths[randomIndex])\n",
    "im7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6a2fdc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0, 121, 215, 130,  73,  73,  73,  73,  47,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0, 198, 253, 253, 254, 253, 253, 253, 247, 236, 183, 145,  83,   0,   0],\n",
       "        [  0,   0,  18,  36,  68, 127,  42,  68,  75, 127, 221, 253, 253, 209,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  16,  97, 199, 254, 242, 143,  31,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  20, 230, 253, 253, 169,  31,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0, 204, 254, 254, 254, 255, 254, 208, 125,  47,   0],\n",
       "        [  0,   0,   0,   0,   0,   0, 185, 218, 198, 133, 109, 108, 186, 222, 247, 136],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  13,   0,   0,   0,   0,   0,  16, 145, 243],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  20, 143],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  47],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  73],\n",
       "        [  0,   0,   0,  46, 117,  66,   0,   0,   0,   0,   0,   0,   0,   0,   0, 170],\n",
       "        [  0,   0,   0, 167, 254, 182,   0,   0,   0,   0,   0,   0,   0,   0, 149, 234],\n",
       "        [  0,   0,   0, 128, 253, 249, 131,   8,   0,   0,   0,   0,   8, 132, 248, 233],\n",
       "        [  0,   0,   0,  10, 158, 254, 253, 209,  56,  37,  37,  37, 164, 253, 207,  58],\n",
       "        [  0,   0,   0,   0,   0, 203, 251, 253, 253, 253, 254, 253, 253, 245, 119,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  87, 175, 253, 253, 254, 201, 149,  41,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(im3)[4:30,4:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f3353",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "98a3a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = [tensor(Image.open(x)) for x in threePaths ]\n",
    "sevens = [tensor(Image.open(x)) for x in sevenPaths ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2c3ae8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131 6265\n"
     ]
    }
   ],
   "source": [
    "print(len(threes), len(sevens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "37c15f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(threes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7403a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_threes = torch.stack(threes).float()\n",
    "stacked_sevens = torch.stack(sevens).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a490dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(255.)\n",
      "tensor(0.) tensor(255.)\n"
     ]
    }
   ],
   "source": [
    "# confirm that pixel values are in the range 0-255 (i.e. 256 range)\n",
    "print(stacked_threes.min(), stacked_threes.max())\n",
    "print(stacked_threes.min(), stacked_sevens.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc71d2",
   "metadata": {},
   "source": [
    "Image pixel values are between 0 and 255 so we can scale it to be between - and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fe52358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_threes = torch.stack(threes).float()/255\n",
    "stacked_sevens = torch.stack(sevens).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "509f851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4e26289e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6131"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stacked_threes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97234d75",
   "metadata": {},
   "source": [
    "We now have all our images stacked up in a single tensor (can imagine a cube stacked up with 2D images). Each image is 28 x 28, and for threes we have 6131 images. Interestingly the len() of these pytorch tensors are the rank (or dimension), i.e. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cde0457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(stacked_threes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c2aca",
   "metadata": {},
   "source": [
    "## compute the \"average\" 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff75556",
   "metadata": {},
   "source": [
    "We simply compute the mean over all images, i.e. mean along the 0 axis (the one of length number of images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cea312a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_three = stacked_threes.mean(0)\n",
    "show_image(mean_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4e15365f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_svn = stacked_sevens.mean(0)\n",
    "show_image(mean_svn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ec57a3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mean_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6da84c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(image: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Classify as 28 x 28 image represented as a pytorch tensor\n",
    "    \n",
    "    Returns the digit which has the least mean squared distance from the image. i.e. we decide if itsa seven or a three \n",
    "    by seeing which \"ideal\" digit its closest to\n",
    "    \"\"\"\n",
    "    distance_three = ((image - mean_three)**2).sum().sqrt()\n",
    "    # or, compute this with the pytorch loss function MSE (mean squared error) \n",
    "    distance_three = F.mse_loss(image, mean_three).sqrt()\n",
    "    distance_svn = F.mse_loss(image, mean_svn).sqrt()\n",
    "    return 0 if distance_svn < distance_three else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affced42",
   "metadata": {},
   "source": [
    "## Evaluate our classifier on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "142389b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testThrees = [tensor(Image.open(p)) for p in (path/'valid'/'3').ls()]\n",
    "testSvns = [tensor(Image.open(p)) for p in (path/'valid'/'7').ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "89089a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = testThrees + testSvns\n",
    "y = tensor(len(testThrees)*[1] + len(testSvns)*[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9c366729",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tensor([classify(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e41738c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 0, 0])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e2286a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9475)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = 1- ((y - y_pred)**2).sum()/len(y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0e36d221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9161)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision on predicting 3 \n",
    "# i.e. out of all the times we predicted 3, what fraction was correct? \n",
    "precision_3 = torch.logical_and(y_pred == 1,y == y_pred).sum() / (y_pred==1).sum()\n",
    "precision_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1b96830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9842)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall on digit 3 \n",
    "# i.e. out of all the true 3 digits, what fraction did we \"hit\"? \n",
    "recall_3 = torch.logical_and(y == 1,y == y_pred).sum() / (y==1).sum()\n",
    "recall_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33820aa9",
   "metadata": {},
   "source": [
    "Our recall is more than 98%, so almost all of the actual 3's we predict correctly. OUr precision on 3 is a bit lower, so that means that we're predicting 3 on some digits that were actually a 7. As you can see below, we are predicting 1085 digits to be a 3 but there are only 1010 true 3s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "887de53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1085)\n",
      "tensor(1010)\n"
     ]
    }
   ],
   "source": [
    "print((y_pred==1).sum())\n",
    "print((y==1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762afa1c",
   "metadata": {},
   "source": [
    "Overall pretty damn impressive for such a naive algorithm! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e2b02",
   "metadata": {},
   "source": [
    "## Vectorize that code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23654ac8",
   "metadata": {},
   "source": [
    "I wrote that off the top of my head, and one bad thing I did is use a python loop (list comprehension) to compute all the predictions in batch on the validation set. Lets rather do this in a vectorized manner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "29fc09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 distanec \n",
    "def mnist_distance(im_a, im_b): return ((im_a - im_b)**2).sum().sqrt()\n",
    "# l1 distance \n",
    "def mnist_distance(im_a,im_b): return (im_a-im_b).abs().mean((-1,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "74ae22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_3s = torch.stack([tensor(Image.open(p)) for p in (path/'valid'/'3').ls()]).float()/255\n",
    "valid_7s = torch.stack([tensor(Image.open(p)) for p in (path/'valid'/'7').ls()]).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0a6e06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_3(image): return mnist_distance(image, mean_three) < mnist_distance(image, mean_svn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7623256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.stack([*valid_3s, *valid_7s])\n",
    "y_val = tensor(len(valid_3s)*[1] + len(valid_7s)*[0]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2e8be623",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = is_3(X_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cb0c68e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9514)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (y_pred == y_val).sum() / len(y_val)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "412f5939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9841)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_3 = torch.logical_and(y_pred == 1,y_val == y_pred).sum() / (y_pred==1).sum()\n",
    "precision_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6c000b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1013,   15],\n",
       "       [  84,  926]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7cf2b",
   "metadata": {},
   "source": [
    "## Training a REAL model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19efff4",
   "metadata": {},
   "source": [
    "lets prepare our training data - we'll flatten each image into a single vector (28x28=784) and use 784 weights, one for each pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a53edf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = [tensor(Image.open(x)) for x in (path/'train'/'3').ls() ]\n",
    "sevens = [tensor(Image.open(x)) for x in (path/'train'/'7').ls() ]\n",
    "stacked_3s = torch.stack(threes).float()/255\n",
    "stacked_7s = torch.stack(sevens).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "35efcc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(stacked_3s.shape)\n",
    "print(stacked_7s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e31a30d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast each 2d image into a 1d array \n",
    "train_x = torch.cat([stacked_3s, stacked_7s]).view(-1,28*28)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d94b1619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in python list1 + list2 concatenates the contents of the two lists\n",
    "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2fbc8901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a dataset of (image, label) tuples \n",
    "dset_train = list(zip(train_x, train_y))\n",
    "x,y = dset_train[0]\n",
    "x.shape, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fb9dfd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2038, 784]), torch.Size([2038, 1]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3s = [tensor(Image.open(x)) for x in (path/'valid'/'3').ls()]\n",
    "valid_7s = [tensor(Image.open(x)) for x in (path/'valid'/'7').ls()]\n",
    "valid_x = torch.cat ([\n",
    "    torch.stack(valid_3s).float()/255,\n",
    "    torch.stack(valid_7s).float()/255\n",
    "]).view(-1,28*28)\n",
    "valid_y = tensor([1]*len(valid_3s) + [0]*len(valid_7s)).unsqueeze(1)\n",
    "valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3396d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_valid = list(zip(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a7b2f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly from a normal distribution around 0 with std=1\n",
    "# remember to mark the parameters as requiring grad, since we will be updating them based on gradients! \n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82db64f",
   "metadata": {},
   "source": [
    "Initialize a model by defining a set of 784 weights (that's 28*28, i.e. a weight for every pixel) plus a bias. We compute the prediction by taking the dot product of an image with the weights vector, and adding the bias (a number). \n",
    "\n",
    "Since weights are pulled from a normal distribution around zero, they are equally likely to push the final prediction to be a positive or a negative number. So we can arbitrarily decide that a positive number means a 3, and a negative number means 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f3e10669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly initialize weights and bias \n",
    "# y = w*x + b\n",
    "weights = init_params([28*28,1])\n",
    "bias = init_params(1)\n",
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "447ce1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.7942], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute a prediction \n",
    "# (+) => 3 , (-) => 7\n",
    "(train_x[0]*weights.T).sum() + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cd430027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.7942], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or do the same with matrix multiplication \n",
    "train_x[0]@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8acf23d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.]]),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         ...,\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compute predictions on the whole batch at once, we can use matrix multiplication\n",
    "def linear1(X): return X@weights + bias\n",
    "\n",
    "preds = linear1(train_x)\n",
    "predicted_y = (preds > 0).float()\n",
    "predicted_y, train_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e990c3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5240)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects = predicted_y == train_y\n",
    "accuracy = corrects.float().sum()/len(corrects)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd427c7",
   "metadata": {},
   "source": [
    "Looks like our accuracy is close to 0.5, i.e. pretty much random, and that's exactly what we would expect for randomly initialized weights\n",
    "\n",
    "Now we need to decide on a loss function and start tuning those weights! First we'll use the sigmoid function to make sure all predictions are between 0 and 1 rather than from [-inf,+inf]. Then we can easily get a distance between a predition (say it's 0.6) and the correct answer, either 0 (3) or 1 (7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6bad6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f352f379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(sigmoid, title=\"Sigmoid\", min=-4,max=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6020ea54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [0.7370],\n",
       "        [0.9835],\n",
       "        ...,\n",
       "        [0.9905],\n",
       "        [1.0000],\n",
       "        [0.3039]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3e6536b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    \"Computes the loss for a whole batch of predictions. We take the mean of the loss for each prediction\"\n",
    "    sigpreds = predictions.sigmoid()\n",
    "    return (targets - sigpreds).abs().mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "79d02d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4748, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_loss(preds, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e9eba",
   "metadata": {},
   "source": [
    "## DataLoader and mini-batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7d82aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 1,  8, 13,  4,  3]),\n",
       " tensor([14,  5, 11,  6,  9]),\n",
       " tensor([10,  0,  7,  2, 12])]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create 15 data points \n",
    "data = range(15)\n",
    "dl = DataLoader(data, batch_size=5, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "315c5e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 12396)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usually you feed a dataloader a dataset of (x,y) tuples where y is the label and x the input. \n",
    "dl_train = DataLoader(dset_train, batch_size=256)\n",
    "len(dl_train), len(dset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3894c2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12396 - 48*256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db6c6e",
   "metadata": {},
   "source": [
    "We have 49 batches of size 256 - that makes space for 49x256=12544 data points, i.e. enough space for our dataset (the last batch is smaller, with 12396 - 48x256 = 108 data points - check for yourself by getting the last batch in the data loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8f62e926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is our first batch of data: \n",
    "x_batch, y_batch = list(dl_train)[0]\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "add6c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another data loader for the validation set \n",
    "dl_valid = DataLoader(dset_valid,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "dd307d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([246, 784])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=list(dl_valid)[-1]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "59c939ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20d12f",
   "metadata": {},
   "source": [
    "## Ok let's try training something using gradients!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d53aebda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually try with a batch size of 4  \n",
    "batch = train_x[:4]\n",
    "batch.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "06b0d534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14.7942],\n",
       "        [ 1.0304],\n",
       "        [ 4.0902],\n",
       "        [ 4.4395]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(batch)\n",
    "preds\n",
    "# remember that sigmoid values are bundled into the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "230ad905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0728, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(preds, train_y[:4])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "523da334",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "91fc2118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0064), tensor([-0.0554]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad.mean(), bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "83f8fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb,yb,model):\n",
    "    \"\"\"\n",
    "    Calculate the gradient on all the parameters implicit in the model. \n",
    "    This modifies the gradient attributes on the relevant parameter objects, and these can be used \n",
    "    to update the params \n",
    "    xb: batch of inputs \n",
    "    yb: batch of labels \n",
    "    model: function that computes (batch of) predictions relative to current param values \n",
    "    \"\"\"\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    # modified the gradient attributes of the weights, which are implicit in the \"model\" definition\n",
    "    # , in our case linear1\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f64644a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0064), tensor([-0.0554]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to zero the gradients before recomputing, otherwise it adds to the gradients\n",
    "# that are currently stored for the weights\n",
    "#  \n",
    "# note: methods ending in _ in pytorch mean they modify the object (\"in-place\" operation)\n",
    "weights.grad.zero_()\n",
    "bias.grad.zero_()\n",
    "calc_grad(train_x[:4], train_y[:4], linear1)\n",
    "weights.grad.mean(), bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b3f94739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    \"\"\"\n",
    "    model: \n",
    "        function that computes the batch of outputs from a batch of inputs. Model weights \n",
    "        are implicit in this function\n",
    "    lr: \n",
    "        learning rate \n",
    "    params: \n",
    "        Each of the (sets of) parameters that update based on gradient. \n",
    "        In our case, they are \"weights\" and \"bias\"  \n",
    "    \"\"\"\n",
    "    for xb, yb in dl_train:\n",
    "        calc_grad(xb,yb,model)\n",
    "        for p in params:\n",
    "            # update all the weights / bias\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "74a28f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(y_preds,y_batch):\n",
    "    preds = y_preds.sigmoid()\n",
    "    correct = (preds > 0.5) == y_batch\n",
    "    # mean of array of 0/1 values is the same as the fraction of values that are 1\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7e6cfb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(linear1(train_x[:4]), train_y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "730f9922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5462200045585632"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function that tells how well our current model is doing on our validation set \n",
    "def validate_model(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in dl_valid]\n",
    "    return torch.stack(accs).mean().item()\n",
    "validate_model(linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "21b59c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6356032490730286"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right now we're doing pretty bad! let's see if training for one epoch helps\n",
    "train_epoch(linear1, lr=1., params=[weights, bias])\n",
    "validate_model(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637d75a",
   "metadata": {},
   "source": [
    "It definitely got better! let's try training for a few more epochs in a loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3fa84a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6518157720565796\n",
      "0.8388076424598694\n",
      "0.905174195766449\n",
      "0.9329863786697388\n",
      "0.9427520036697388\n",
      "0.9510527849197388\n",
      "0.9569121599197388\n",
      "0.960838258266449\n",
      "0.9642760753631592\n",
      "0.9657409191131592\n",
      "0.9686904549598694\n",
      "0.9691787362098694\n",
      "0.9711318612098694\n",
      "0.9721084237098694\n",
      "0.9730849862098694\n",
      "0.9745498299598694\n",
      "0.9750381112098694\n",
      "0.9755263924598694\n",
      "0.9750381112098694\n",
      "0.9750381112098694\n"
     ]
    }
   ],
   "source": [
    "# let's train now for a few more epochs! I'll re-initialize weights just to see the uptrend\n",
    "weights = init_params([28*28,1])\n",
    "bias = init_params(1)\n",
    "for i in range(20):\n",
    "    train_epoch(linear1, lr=1., params=[weights, bias])\n",
    "    acc = validate_model(linear1)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b0b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "13a4cf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AxesSubplot:>, tensor([0]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I want to see it just to be sure \n",
    "rand_index = torch.randint(0,len(dset_valid),[1])[0]\n",
    "sample_x, sample_y = dset_valid[rand_index]\n",
    "show_image(sample_x.reshape([28,28])), sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e530735c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([0]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 => 3, 0 => 7\n",
    "round(float(linear1(sample_x).sigmoid()[0])), sample_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6570ee4",
   "metadata": {},
   "source": [
    "Sure seems to get the correct answer on random validation samples! We get 97% accuracy after 20 epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316760f",
   "metadata": {},
   "source": [
    "## Doing training the fastai way \n",
    "### (using an Optimizer)\n",
    "\n",
    "First off, we use pytorch's built-in linear function instead of manually defining it. This comes with a set of weights and bias built in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0d9b1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ffe2437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b=linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "75749962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definte our own Optimizer class \n",
    "\n",
    "class BasicOptimizer:\n",
    "    def __init__(self, params, lr): \n",
    "        self.params, self.lr = list(params), lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.data -= p.grad.data * self.lr\n",
    "    \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1fbfa6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer class\n",
    "opt = BasicOptimizer(linear_model.parameters(), lr=1.)\n",
    "# training loop can now be written like this: \n",
    "def train_epoch(model, optimizer):\n",
    "    for xb, yb in dl_train:\n",
    "        calc_grad(xb,yb,model)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "72802f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, n_epochs):\n",
    "    for i in range(n_epochs):\n",
    "        train_epoch(model, optimizer)\n",
    "        print(validate_model(model), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8a0aa99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4931640625 0.6996871829032898 0.8554489016532898 0.9116012454032898 0.9360153079032898 0.9516403079032898 0.9594528079032898 0.9648239016532898 0.9667770266532898 0.9687301516532898 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = BasicOptimizer(linear_model.parameters(), lr=1.)\n",
    "train_model(linear_model, opt, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb7d5e",
   "metadata": {},
   "source": [
    "We wrote the optimizer class ourselves in this intance but, you guessed it, fastai provides built-in optimizers like this. For example, _stochastic_ gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "036e2527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4931640625 0.7372848391532898 0.8534957766532898 0.9120895266532898 0.9365035891532898 0.9511520266532898 0.9609176516532898 0.9653121829032898 0.9667770266532898 0.9687301516532898 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "optimizer = SGD(linear_model.parameters(), lr=1.)\n",
    "train_model(linear_model, optimizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85753011",
   "metadata": {},
   "source": [
    "fastai also provides a class to do the training (fitting), so let's use that rather than our train_model function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "cdc5e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl_train, dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "82f4e5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "These elements are needed to train a model: \n",
    "\n",
    "data loader (dl):\n",
    "    Stores a dataset (e.g. training set) and breaks it up into batches for training \n",
    "\n",
    "data loaders (dls):\n",
    "    A set of data loaders, typically training and validation datasets \n",
    "\n",
    "model: \n",
    "    A function that computes the predicted value. \n",
    "    It depends on a set of parameters that have gradient enabled, so that gradient-descent \n",
    "    based parameter updates can be made\n",
    "\n",
    "optimizer:\n",
    "    Updates the model parameters based on the gradients. Many different trategies are possible here - \n",
    "    different learning rates, using randomness, etc. \n",
    "\n",
    "loss function:\n",
    "    Function that we compute the gradient on, with respect to the parameters. Given a particular \n",
    "    (input, target) pair together with a set of parameter values, we can compute the loss by comparing\n",
    "    the prediction with the target. There are may different loss functions, e.g. euclidean distance, l1 \n",
    "    distance, etc.  \n",
    "\n",
    "metrics: \n",
    "    Performance function such as accuracy.\n",
    "\"\"\" \n",
    "0 # ignore me I'm just a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4cdacf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "58466232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.636460</td>\n",
       "      <td>0.499574</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258525</td>\n",
       "      <td>0.323309</td>\n",
       "      <td>0.667812</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104868</td>\n",
       "      <td>0.159392</td>\n",
       "      <td>0.858194</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052530</td>\n",
       "      <td>0.099056</td>\n",
       "      <td>0.914622</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032892</td>\n",
       "      <td>0.072773</td>\n",
       "      <td>0.936703</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.024948</td>\n",
       "      <td>0.058306</td>\n",
       "      <td>0.951423</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>0.049516</td>\n",
       "      <td>0.960746</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019572</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>0.964671</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018459</td>\n",
       "      <td>0.039940</td>\n",
       "      <td>0.966634</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.037049</td>\n",
       "      <td>0.968597</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017115</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.969087</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016670</td>\n",
       "      <td>0.032984</td>\n",
       "      <td>0.971541</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.016313</td>\n",
       "      <td>0.031482</td>\n",
       "      <td>0.973503</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.016016</td>\n",
       "      <td>0.030218</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.015760</td>\n",
       "      <td>0.029142</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.015531</td>\n",
       "      <td>0.028216</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.015323</td>\n",
       "      <td>0.027412</td>\n",
       "      <td>0.977920</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.015131</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.014784</td>\n",
       "      <td>0.025534</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(20, lr=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89376f9",
   "metadata": {},
   "source": [
    "So fastai provides a bunch of classes that means you can get going with training really quickly and easily "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a8d90",
   "metadata": {},
   "source": [
    "So far the function we have for predicting 3 vs 7 is literally some linear combination of all the pixels, plus a bias. That is, its totally linear. \n",
    "\n",
    "What if we introduce some nonlinearity? Could that up our accuracy? With a naive approach we achieved ~94%, and with the linear model weights we achieved 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ec6819c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer creates 30 different features, which then gets fed to the next layer. \n",
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1) \n",
    "\n",
    "\n",
    "def simple_net(xb):\n",
    "    \"\"\" \n",
    "    Simple neural net with a single activation layer. \n",
    "    \"\"\"\n",
    "    # res.shape = (len(xb),30) -> 30 features for each input in the batch  \n",
    "    res = xb@w1 + b1\n",
    "    # this is ReLU right here! \n",
    "    # It takes any negative number to 0 (kills the info), \n",
    "    # and any positive number to itself (preserves the info) \n",
    "    res = res.max(tensor(0.0))\n",
    "    # res.shape = len(xb) -> a single output for each input in the batch \n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a1a16b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(F.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8414d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our simple neural net can be replacedby the following in pytorch \n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28, 30),\n",
    "    # note that relu just acts on each element so doesn't change the dimensions\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8f9c49c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2090],\n",
       "        [-0.0006],\n",
       "        [-0.0695]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. compute the output for 3 random samples (images) \n",
    "random_samples = torch.randn([3,28*28])\n",
    "print(random_samples.shape)\n",
    "simple_net(random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "db4b2b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.292510</td>\n",
       "      <td>0.415099</td>\n",
       "      <td>0.504416</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.139134</td>\n",
       "      <td>0.216396</td>\n",
       "      <td>0.818940</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.078467</td>\n",
       "      <td>0.110397</td>\n",
       "      <td>0.919038</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052676</td>\n",
       "      <td>0.075343</td>\n",
       "      <td>0.943572</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040545</td>\n",
       "      <td>0.059212</td>\n",
       "      <td>0.957802</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034154</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.963690</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030356</td>\n",
       "      <td>0.044305</td>\n",
       "      <td>0.966143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027834</td>\n",
       "      <td>0.040309</td>\n",
       "      <td>0.967615</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026009</td>\n",
       "      <td>0.037387</td>\n",
       "      <td>0.968106</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024606</td>\n",
       "      <td>0.035148</td>\n",
       "      <td>0.970559</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.033375</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.031927</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.030716</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.021106</td>\n",
       "      <td>0.029682</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.028786</td>\n",
       "      <td>0.976448</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019994</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.027303</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.019096</td>\n",
       "      <td>0.026678</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018705</td>\n",
       "      <td>0.026114</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018344</td>\n",
       "      <td>0.025601</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now lets try learn this problem! \n",
    "learn = Learner(dls, simple_net, loss_func=mnist_loss, opt_func=SGD, metrics=batch_accuracy)\n",
    "learn.fit(20, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "38f69d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0a4af92670>]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L(learn.recorder.values).itemgot(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fd83fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e7c1d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchristos/.local/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n",
      "  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>None</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/32 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Exception occured in `Recorder` when calling event `after_batch`:\n\t'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mchristos/code/fastbook/04_mnist_basics[chris].ipynb Cell 124'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/mchristos/code/fastbook/04_mnist_basics%5Bchris%5D.ipynb#ch0000139vscode-remote?line=0'>1</a>\u001b[0m learn \u001b[39m=\u001b[39m cnn_learner(dls, resnet18, pretrained\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, loss_func\u001b[39m=\u001b[39mF\u001b[39m.\u001b[39mcross_entropy, metrics\u001b[39m=\u001b[39maccuracy)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/mchristos/code/fastbook/04_mnist_basics%5Bchris%5D.ipynb#ch0000139vscode-remote?line=1'>2</a>\u001b[0m learn\u001b[39m.\u001b[39;49mfit_one_cycle(\u001b[39m1\u001b[39;49m, \u001b[39m0.1\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/callback/schedule.py:116\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    113\u001b[0m lr_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([h[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mhypers])\n\u001b[1;32m    114\u001b[0m scheds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[39m/\u001b[39mdiv, lr_max, lr_max\u001b[39m/\u001b[39mdiv_final),\n\u001b[1;32m    115\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mmom\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, \u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoms \u001b[39mif\u001b[39;00m moms \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m moms))}\n\u001b[0;32m--> 116\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(n_epoch, cbs\u001b[39m=\u001b[39;49mParamScheduler(scheds)\u001b[39m+\u001b[39;49mL(cbs), reset_opt\u001b[39m=\u001b[39;49mreset_opt, wd\u001b[39m=\u001b[39;49mwd)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:221\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mset_hypers(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39mif\u001b[39;00m lr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lr)\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch \u001b[39m=\u001b[39m n_epoch\n\u001b[0;32m--> 221\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_fit, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelFitException, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_end_cleanup)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    164\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:212\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch):\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m=\u001b[39mepoch\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch, \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelEpochException)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    164\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_epoch_train()\n\u001b[0;32m--> 207\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch_validate()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:203\u001b[0m, in \u001b[0;36mLearner._do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mif\u001b[39;00m dl \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: dl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls[ds_idx]\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m dl\n\u001b[0;32m--> 203\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_batches, \u001b[39m'\u001b[39;49m\u001b[39mvalidate\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelValidException)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:163\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 163\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    164\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:169\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_batch(\u001b[39m*\u001b[39;49mo)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:194\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    192\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_device(b)\n\u001b[1;32m    193\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split(b)\n\u001b[0;32m--> 194\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_one_batch, \u001b[39m'\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBatchException)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:165\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[39mself\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mafter_\u001b[39;49m\u001b[39m{\u001b[39;49;00mevent_type\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m);  final()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:141\u001b[0m, in \u001b[0;36mLearner.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[0;32m--> 141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, event_name): L(event_name)\u001b[39m.\u001b[39;49mmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastcore/foundation.py:155\u001b[0m, in \u001b[0;36mL.map\u001b[0;34m(self, f, gen, *args, **kwargs)\u001b[0m\n\u001b[0;32m--> 155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, f, \u001b[39m*\u001b[39margs, gen\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(map_ex(\u001b[39mself\u001b[39;49m, f, \u001b[39m*\u001b[39;49margs, gen\u001b[39m=\u001b[39;49mgen, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastcore/basics.py:698\u001b[0m, in \u001b[0;36mmap_ex\u001b[0;34m(iterable, f, gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(g, iterable)\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m gen: \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m--> 698\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(res)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastcore/basics.py:683\u001b[0m, in \u001b[0;36mbind.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v,_Arg): kwargs[k] \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mpop(v\u001b[39m.\u001b[39mi)\n\u001b[1;32m    682\u001b[0m fargs \u001b[39m=\u001b[39m [args[x\u001b[39m.\u001b[39mi] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, _Arg) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpargs] \u001b[39m+\u001b[39m args[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 683\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49mfargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:145\u001b[0m, in \u001b[0;36mLearner._call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_one\u001b[39m(\u001b[39mself\u001b[39m, event_name):\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(event, event_name): \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmissing \u001b[39m\u001b[39m{\u001b[39;00mevent_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcbs\u001b[39m.\u001b[39msorted(\u001b[39m'\u001b[39m\u001b[39morder\u001b[39m\u001b[39m'\u001b[39m): cb(event_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/callback/core.py:57\u001b[0m, in \u001b[0;36mCallback.__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     55\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun \u001b[39mand\u001b[39;00m _run:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mtry\u001b[39;00m: res \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, event_name, noop)()\n\u001b[1;32m     58\u001b[0m     \u001b[39mexcept\u001b[39;00m (CancelBatchException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:511\u001b[0m, in \u001b[0;36mRecorder.after_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myb) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    510\u001b[0m mets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_mets \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_mets\n\u001b[0;32m--> 511\u001b[0m \u001b[39mfor\u001b[39;00m met \u001b[39min\u001b[39;00m mets: met\u001b[39m.\u001b[39;49maccumulate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn)\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining: \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlrs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mhypers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastai/learner.py:433\u001b[0m, in \u001b[0;36mAvgMetric.accumulate\u001b[0;34m(self, learn)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccumulate\u001b[39m(\u001b[39mself\u001b[39m, learn):\n\u001b[1;32m    432\u001b[0m     bs \u001b[39m=\u001b[39m find_bs(learn\u001b[39m.\u001b[39myb)\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learn\u001b[39m.\u001b[39mto_detach(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(learn\u001b[39m.\u001b[39;49mpred, \u001b[39m*\u001b[39;49mlearn\u001b[39m.\u001b[39;49myb))\u001b[39m*\u001b[39mbs\n\u001b[1;32m    434\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m bs\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception occured in `Recorder` when calling event `after_batch`:\n\t'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8b225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
